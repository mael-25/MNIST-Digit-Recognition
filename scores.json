{
    "Logs/MNIST-epochs=50-layers=7-10.pt": {
        "score": 0.9843,
        "max-score": 0.9853,
        "max-score-epoch": 25,
        "epochs": 50,
        "validation-results": {
            "1": 0.9334,
            "2": 0.9453,
            "3": 0.962,
            "4": 0.9576,
            "5": 0.9722,
            "6": 0.9712,
            "7": 0.9715,
            "8": 0.9703,
            "9": 0.9731,
            "10": 0.978,
            "11": 0.9753,
            "12": 0.9787,
            "13": 0.9736,
            "14": 0.9779,
            "15": 0.9747,
            "16": 0.9759,
            "17": 0.9726,
            "18": 0.9768,
            "19": 0.9781,
            "20": 0.9833,
            "21": 0.9828,
            "22": 0.9818,
            "23": 0.9832,
            "24": 0.9837,
            "25": 0.9853,
            "26": 0.9851,
            "27": 0.983,
            "28": 0.9847,
            "29": 0.9827,
            "30": 0.9846,
            "31": 0.9843,
            "32": 0.9845,
            "33": 0.9846,
            "34": 0.9851,
            "35": 0.9842,
            "36": 0.9847,
            "37": 0.9848,
            "38": 0.9849,
            "39": 0.9852,
            "40": 0.9847,
            "41": 0.9847,
            "42": 0.9845,
            "43": 0.983,
            "44": 0.9843,
            "45": 0.9838,
            "46": 0.9838,
            "47": 0.9845,
            "48": 0.9848,
            "49": 0.9847,
            "50": 0.9843
        },
        "learning-rate": {
            "1": 0.01,
            "5": 0.0075,
            "10": 0.005,
            "20": 0.003,
            "25": 0.0025,
            "30": 0.001,
            "45": 0.0005
        },
        "batch-size": 32,
        "weight-decay": 0.0005,
        "model-class-type": "linear",
        "model": "Sequential(\n  (0): Linear(in_features=784, out_features=512, bias=True)\n  (1): Dropout(p=0.00075, inplace=False)\n  (2): ReLU()\n  (3): Linear(in_features=512, out_features=256, bias=True)\n  (4): Dropout(p=0.00075, inplace=False)\n  (5): ReLU()\n  (6): Linear(in_features=256, out_features=128, bias=True)\n  (7): Dropout(p=0.00075, inplace=False)\n  (8): ReLU()\n  (9): Linear(in_features=128, out_features=64, bias=True)\n  (10): Dropout(p=0.00075, inplace=False)\n  (11): ReLU()\n  (12): Linear(in_features=64, out_features=32, bias=True)\n  (13): Dropout(p=0.00075, inplace=False)\n  (14): ReLU()\n  (15): Linear(in_features=32, out_features=16, bias=True)\n  (16): Dropout(p=0.00075, inplace=False)\n  (17): ReLU()\n  (18): Linear(in_features=16, out_features=10, bias=True)\n  (19): LogSoftmax(dim=1)\n)"
    },
    "Logs/MNIST-epochs=100-layers=7-4.pt": {
        "score": 0.9838,
        "max-score": 0.9853,
        "max-score-epoch": 34,
        "epochs": 100,
        "validation-results": {
            "1": 0.9154,
            "2": 0.9481,
            "3": 0.953,
            "4": 0.965,
            "5": 0.971,
            "6": 0.9747,
            "7": 0.9625,
            "8": 0.9746,
            "9": 0.9765,
            "10": 0.9754,
            "11": 0.9738,
            "12": 0.9782,
            "13": 0.9789,
            "14": 0.9755,
            "15": 0.9806,
            "16": 0.9791,
            "17": 0.9814,
            "18": 0.9789,
            "19": 0.9805,
            "20": 0.9825,
            "21": 0.9839,
            "22": 0.9845,
            "23": 0.9841,
            "24": 0.9823,
            "25": 0.9838,
            "26": 0.9843,
            "27": 0.9847,
            "28": 0.9841,
            "29": 0.9843,
            "30": 0.9844,
            "31": 0.9851,
            "32": 0.9843,
            "33": 0.9849,
            "34": 0.9853,
            "35": 0.985,
            "36": 0.9845,
            "37": 0.9841,
            "38": 0.9845,
            "39": 0.9845,
            "40": 0.9845,
            "41": 0.9844,
            "42": 0.9846,
            "43": 0.9849,
            "44": 0.9844,
            "45": 0.9845,
            "46": 0.9847,
            "47": 0.9848,
            "48": 0.9845,
            "49": 0.9845,
            "50": 0.9847,
            "51": 0.984,
            "52": 0.9848,
            "53": 0.9844,
            "54": 0.9847,
            "55": 0.9847,
            "56": 0.9847,
            "57": 0.9845,
            "58": 0.9845,
            "59": 0.9842,
            "60": 0.9846,
            "61": 0.9842,
            "62": 0.9841,
            "63": 0.9838,
            "64": 0.9847,
            "65": 0.9842,
            "66": 0.9847,
            "67": 0.9841,
            "68": 0.9845,
            "69": 0.9839,
            "70": 0.9839,
            "71": 0.984,
            "72": 0.9843,
            "73": 0.9843,
            "74": 0.9841,
            "75": 0.9846,
            "76": 0.984,
            "77": 0.9837,
            "78": 0.9837,
            "79": 0.9847,
            "80": 0.984,
            "81": 0.9842,
            "82": 0.9844,
            "83": 0.9839,
            "84": 0.9838,
            "85": 0.9842,
            "86": 0.9836,
            "87": 0.9841,
            "88": 0.9841,
            "89": 0.9839,
            "90": 0.9842,
            "91": 0.9836,
            "92": 0.9847,
            "93": 0.9838,
            "94": 0.9843,
            "95": 0.9836,
            "96": 0.9846,
            "97": 0.9843,
            "98": 0.984,
            "99": 0.9847,
            "100": 0.9838
        },
        "learning-rate": {
            "1": 0.01,
            "5": 0.0075,
            "10": 0.005,
            "20": 0.003,
            "25": 0.0025,
            "30": 0.001,
            "45": 0.0005
        },
        "batch-size": 32,
        "weight-decay": 0.0005,
        "model-class-type": "linear",
        "model": "Sequential(\n  (0): Linear(in_features=784, out_features=512, bias=True)\n  (1): Dropout(p=0.00075, inplace=False)\n  (2): ReLU()\n  (3): Linear(in_features=512, out_features=256, bias=True)\n  (4): Dropout(p=0.00075, inplace=False)\n  (5): ReLU()\n  (6): Linear(in_features=256, out_features=128, bias=True)\n  (7): Dropout(p=0.00075, inplace=False)\n  (8): ReLU()\n  (9): Linear(in_features=128, out_features=64, bias=True)\n  (10): Dropout(p=0.00075, inplace=False)\n  (11): ReLU()\n  (12): Linear(in_features=64, out_features=32, bias=True)\n  (13): Dropout(p=0.00075, inplace=False)\n  (14): ReLU()\n  (15): Linear(in_features=32, out_features=16, bias=True)\n  (16): Dropout(p=0.00075, inplace=False)\n  (17): ReLU()\n  (18): Linear(in_features=16, out_features=10, bias=True)\n  (19): LogSoftmax(dim=1)\n)"
    },
    "Logs/MNIST-epochs=100-layers=7-2.pt": {
        "score": 0.9758,
        "max-score": 0.9789,
        "max-score-epoch": 73,
        "epochs": 100,
        "validation-results": {
            "1": 0.8579,
            "2": 0.8884,
            "3": 0.9184,
            "4": 0.9334,
            "5": 0.9207,
            "6": 0.9321,
            "7": 0.9449,
            "8": 0.9354,
            "9": 0.9257,
            "10": 0.9603,
            "11": 0.9641,
            "12": 0.9451,
            "13": 0.96,
            "14": 0.9599,
            "15": 0.9429,
            "16": 0.9613,
            "17": 0.9634,
            "18": 0.9644,
            "19": 0.9412,
            "20": 0.9665,
            "21": 0.9659,
            "22": 0.9665,
            "23": 0.9529,
            "24": 0.9635,
            "25": 0.9707,
            "26": 0.9569,
            "27": 0.9677,
            "28": 0.9646,
            "29": 0.9728,
            "30": 0.9778,
            "31": 0.9773,
            "32": 0.9759,
            "33": 0.9748,
            "34": 0.9749,
            "35": 0.9725,
            "36": 0.9713,
            "37": 0.975,
            "38": 0.9754,
            "39": 0.9735,
            "40": 0.9761,
            "41": 0.9721,
            "42": 0.9773,
            "43": 0.9761,
            "44": 0.9762,
            "45": 0.9784,
            "46": 0.9777,
            "47": 0.9773,
            "48": 0.9777,
            "49": 0.9761,
            "50": 0.9757,
            "51": 0.9772,
            "52": 0.9769,
            "53": 0.9774,
            "54": 0.975,
            "55": 0.976,
            "56": 0.9752,
            "57": 0.977,
            "58": 0.9781,
            "59": 0.9776,
            "60": 0.9779,
            "61": 0.9769,
            "62": 0.9757,
            "63": 0.9753,
            "64": 0.9703,
            "65": 0.9755,
            "66": 0.9763,
            "67": 0.9726,
            "68": 0.9776,
            "69": 0.9764,
            "70": 0.9768,
            "71": 0.9759,
            "72": 0.9739,
            "73": 0.9789,
            "74": 0.977,
            "75": 0.9773,
            "76": 0.9759,
            "77": 0.9744,
            "78": 0.9766,
            "79": 0.9769,
            "80": 0.9778,
            "81": 0.9779,
            "82": 0.9761,
            "83": 0.9719,
            "84": 0.9759,
            "85": 0.9772,
            "86": 0.975,
            "87": 0.975,
            "88": 0.9776,
            "89": 0.9754,
            "90": 0.9754,
            "91": 0.9774,
            "92": 0.9779,
            "93": 0.9756,
            "94": 0.9763,
            "95": 0.9702,
            "96": 0.9707,
            "97": 0.9776,
            "98": 0.9744,
            "99": 0.974,
            "100": 0.9758
        },
        "learning-rate": {
            "1": 0.01,
            "5": 0.0075,
            "10": 0.005,
            "20": 0.003,
            "25": 0.0025,
            "30": 0.001,
            "45": 0.0005
        },
        "batch-size": 32,
        "weight-decay": 0.005,
        "model-class-type": "linear",
        "model": "Sequential(\n  (0): Linear(in_features=784, out_features=512, bias=True)\n  (1): Dropout(p=0.005, inplace=False)\n  (2): ReLU()\n  (3): Linear(in_features=512, out_features=256, bias=True)\n  (4): Dropout(p=0.005, inplace=False)\n  (5): ReLU()\n  (6): Linear(in_features=256, out_features=128, bias=True)\n  (7): Dropout(p=0.005, inplace=False)\n  (8): ReLU()\n  (9): Linear(in_features=128, out_features=64, bias=True)\n  (10): Dropout(p=0.005, inplace=False)\n  (11): ReLU()\n  (12): Linear(in_features=64, out_features=32, bias=True)\n  (13): Dropout(p=0.005, inplace=False)\n  (14): ReLU()\n  (15): Linear(in_features=32, out_features=16, bias=True)\n  (16): Dropout(p=0.005, inplace=False)\n  (17): ReLU()\n  (18): Linear(in_features=16, out_features=10, bias=True)\n  (19): LogSoftmax(dim=1)\n)"
    },
    "Logs/MNIST-epochs=75-layers=7-3.pt": {
        "score": 0.9768,
        "max-score": 0.9787,
        "max-score-epoch": 66,
        "epochs": 75,
        "validation-results": {
            "1": 0.8689,
            "2": 0.9217,
            "3": 0.9408,
            "4": 0.9064,
            "5": 0.9285,
            "6": 0.9495,
            "7": 0.9139,
            "8": 0.9469,
            "9": 0.95,
            "10": 0.9592,
            "11": 0.9459,
            "12": 0.9565,
            "13": 0.9211,
            "14": 0.9586,
            "15": 0.9522,
            "16": 0.9562,
            "17": 0.9572,
            "18": 0.9543,
            "19": 0.9661,
            "20": 0.9677,
            "21": 0.964,
            "22": 0.9679,
            "23": 0.9688,
            "24": 0.9681,
            "25": 0.9712,
            "26": 0.9669,
            "27": 0.9674,
            "28": 0.9656,
            "29": 0.9555,
            "30": 0.9756,
            "31": 0.9746,
            "32": 0.9723,
            "33": 0.977,
            "34": 0.9755,
            "35": 0.9749,
            "36": 0.9767,
            "37": 0.9752,
            "38": 0.975,
            "39": 0.9738,
            "40": 0.9767,
            "41": 0.9662,
            "42": 0.977,
            "43": 0.975,
            "44": 0.9698,
            "45": 0.9781,
            "46": 0.9765,
            "47": 0.9776,
            "48": 0.9786,
            "49": 0.9773,
            "50": 0.9777,
            "51": 0.9737,
            "52": 0.9786,
            "53": 0.9775,
            "54": 0.9779,
            "55": 0.975,
            "56": 0.9773,
            "57": 0.9763,
            "58": 0.9757,
            "59": 0.9764,
            "60": 0.9776,
            "61": 0.9743,
            "62": 0.9772,
            "63": 0.976,
            "64": 0.976,
            "65": 0.9784,
            "66": 0.9787,
            "67": 0.9693,
            "68": 0.9766,
            "69": 0.9777,
            "70": 0.9774,
            "71": 0.9735,
            "72": 0.9738,
            "73": 0.9781,
            "74": 0.9721,
            "75": 0.9768
        },
        "learning-rate": {
            "1": 0.01,
            "5": 0.0075,
            "10": 0.005,
            "20": 0.003,
            "25": 0.0025,
            "30": 0.001,
            "45": 0.0005
        },
        "batch-size": 32,
        "weight-decay": 0.005,
        "model-class-type": "linear",
        "model": "Sequential(\n  (0): Linear(in_features=784, out_features=512, bias=True)\n  (1): Dropout(p=0.005, inplace=False)\n  (2): ReLU()\n  (3): Linear(in_features=512, out_features=256, bias=True)\n  (4): Dropout(p=0.005, inplace=False)\n  (5): ReLU()\n  (6): Linear(in_features=256, out_features=128, bias=True)\n  (7): Dropout(p=0.005, inplace=False)\n  (8): ReLU()\n  (9): Linear(in_features=128, out_features=64, bias=True)\n  (10): Dropout(p=0.005, inplace=False)\n  (11): ReLU()\n  (12): Linear(in_features=64, out_features=32, bias=True)\n  (13): Dropout(p=0.005, inplace=False)\n  (14): ReLU()\n  (15): Linear(in_features=32, out_features=16, bias=True)\n  (16): Dropout(p=0.005, inplace=False)\n  (17): ReLU()\n  (18): Linear(in_features=16, out_features=10, bias=True)\n  (19): LogSoftmax(dim=1)\n)"
    },
    "Logs/MNIST-epochs=50-layers=7-9.pt": {
        "score": 0.9754,
        "max-score": 0.9778,
        "max-score-epoch": 49,
        "epochs": 50,
        "validation-results": {
            "1": 0.8772,
            "2": 0.903,
            "3": 0.9246,
            "4": 0.9451,
            "5": 0.949,
            "6": 0.9389,
            "7": 0.9258,
            "8": 0.9385,
            "9": 0.9499,
            "10": 0.9474,
            "11": 0.9497,
            "12": 0.9609,
            "13": 0.9447,
            "14": 0.9526,
            "15": 0.9454,
            "16": 0.9568,
            "17": 0.9607,
            "18": 0.9577,
            "19": 0.9431,
            "20": 0.9613,
            "21": 0.9632,
            "22": 0.9655,
            "23": 0.9549,
            "24": 0.9627,
            "25": 0.9636,
            "26": 0.9659,
            "27": 0.9699,
            "28": 0.9736,
            "29": 0.9615,
            "30": 0.9743,
            "31": 0.9745,
            "32": 0.9736,
            "33": 0.9749,
            "34": 0.9674,
            "35": 0.9712,
            "36": 0.973,
            "37": 0.9754,
            "38": 0.9742,
            "39": 0.9711,
            "40": 0.9719,
            "41": 0.9742,
            "42": 0.9731,
            "43": 0.972,
            "44": 0.9737,
            "45": 0.9744,
            "46": 0.9768,
            "47": 0.9757,
            "48": 0.9737,
            "49": 0.9778,
            "50": 0.9754
        },
        "learning-rate": {
            "1": 0.01,
            "5": 0.0075,
            "10": 0.005,
            "20": 0.003,
            "25": 0.0025,
            "30": 0.001,
            "45": 0.0005
        },
        "batch-size": 32,
        "weight-decay": 0.005,
        "model-class-type": "linear",
        "model": "Sequential(\n  (0): Linear(in_features=784, out_features=512, bias=True)\n  (1): Dropout(p=0.005, inplace=False)\n  (2): ReLU()\n  (3): Linear(in_features=512, out_features=256, bias=True)\n  (4): Dropout(p=0.005, inplace=False)\n  (5): ReLU()\n  (6): Linear(in_features=256, out_features=128, bias=True)\n  (7): Dropout(p=0.005, inplace=False)\n  (8): ReLU()\n  (9): Linear(in_features=128, out_features=64, bias=True)\n  (10): Dropout(p=0.005, inplace=False)\n  (11): ReLU()\n  (12): Linear(in_features=64, out_features=32, bias=True)\n  (13): Dropout(p=0.005, inplace=False)\n  (14): ReLU()\n  (15): Linear(in_features=32, out_features=16, bias=True)\n  (16): Dropout(p=0.005, inplace=False)\n  (17): ReLU()\n  (18): Linear(in_features=16, out_features=10, bias=True)\n  (19): LogSoftmax(dim=1)\n)"
    },
    "Logs/MNIST-epochs=100-layers=7-3.pt": {
        "score": 0.9717,
        "max-score": 0.9776,
        "max-score-epoch": 49,
        "epochs": 100,
        "validation-results": {
            "1": 0.8496,
            "2": 0.9084,
            "3": 0.9105,
            "4": 0.9215,
            "5": 0.9444,
            "6": 0.9145,
            "7": 0.9383,
            "8": 0.949,
            "9": 0.9193,
            "10": 0.942,
            "11": 0.957,
            "12": 0.9482,
            "13": 0.932,
            "14": 0.9606,
            "15": 0.9579,
            "16": 0.9659,
            "17": 0.9554,
            "18": 0.9611,
            "19": 0.962,
            "20": 0.9621,
            "21": 0.9682,
            "22": 0.9654,
            "23": 0.9651,
            "24": 0.9595,
            "25": 0.9679,
            "26": 0.9644,
            "27": 0.9675,
            "28": 0.9659,
            "29": 0.9679,
            "30": 0.9769,
            "31": 0.9736,
            "32": 0.9761,
            "33": 0.9759,
            "34": 0.9753,
            "35": 0.9706,
            "36": 0.9693,
            "37": 0.9738,
            "38": 0.973,
            "39": 0.9762,
            "40": 0.9759,
            "41": 0.9719,
            "42": 0.9764,
            "43": 0.9732,
            "44": 0.9719,
            "45": 0.9742,
            "46": 0.9769,
            "47": 0.9766,
            "48": 0.9774,
            "49": 0.9776,
            "50": 0.9745,
            "51": 0.9758,
            "52": 0.9757,
            "53": 0.9776,
            "54": 0.9744,
            "55": 0.9774,
            "56": 0.9765,
            "57": 0.974,
            "58": 0.9763,
            "59": 0.9734,
            "60": 0.9764,
            "61": 0.9712,
            "62": 0.9764,
            "63": 0.9767,
            "64": 0.976,
            "65": 0.9765,
            "66": 0.9715,
            "67": 0.9756,
            "68": 0.9741,
            "69": 0.9736,
            "70": 0.9751,
            "71": 0.9767,
            "72": 0.9719,
            "73": 0.9762,
            "74": 0.9745,
            "75": 0.9745,
            "76": 0.9718,
            "77": 0.9754,
            "78": 0.9705,
            "79": 0.9746,
            "80": 0.9752,
            "81": 0.9722,
            "82": 0.9739,
            "83": 0.9753,
            "84": 0.9751,
            "85": 0.9755,
            "86": 0.976,
            "87": 0.9759,
            "88": 0.9755,
            "89": 0.9756,
            "90": 0.9758,
            "91": 0.973,
            "92": 0.9753,
            "93": 0.9745,
            "94": 0.9765,
            "95": 0.9742,
            "96": 0.9764,
            "97": 0.9712,
            "98": 0.9724,
            "99": 0.9739,
            "100": 0.9717
        },
        "learning-rate": {
            "1": 0.01,
            "5": 0.0075,
            "10": 0.005,
            "20": 0.003,
            "25": 0.0025,
            "30": 0.001,
            "45": 0.0005
        },
        "batch-size": 32,
        "weight-decay": 0.005,
        "model-class-type": "linear",
        "model": "Sequential(\n  (0): Linear(in_features=784, out_features=512, bias=True)\n  (1): Dropout(p=0.005, inplace=False)\n  (2): ReLU()\n  (3): Linear(in_features=512, out_features=256, bias=True)\n  (4): Dropout(p=0.005, inplace=False)\n  (5): ReLU()\n  (6): Linear(in_features=256, out_features=128, bias=True)\n  (7): Dropout(p=0.005, inplace=False)\n  (8): ReLU()\n  (9): Linear(in_features=128, out_features=64, bias=True)\n  (10): Dropout(p=0.005, inplace=False)\n  (11): ReLU()\n  (12): Linear(in_features=64, out_features=32, bias=True)\n  (13): Dropout(p=0.005, inplace=False)\n  (14): ReLU()\n  (15): Linear(in_features=32, out_features=16, bias=True)\n  (16): Dropout(p=0.005, inplace=False)\n  (17): ReLU()\n  (18): Linear(in_features=16, out_features=10, bias=True)\n  (19): LogSoftmax(dim=1)\n)"
    },
    "Logs/MNIST-epochs=10-layers=7-1.pt": {
        "score": 0.9536,
        "max-score": 0.9536,
        "max-score-epoch": 10,
        "epochs": 10,
        "validation-results": {
            "1": 0.8759,
            "2": 0.9263,
            "3": 0.9156,
            "4": 0.9388,
            "5": 0.9462,
            "6": 0.9365,
            "7": 0.9386,
            "8": 0.9471,
            "9": 0.9355,
            "10": 0.9536
        },
        "learning-rate": {
            "1": 0.01,
            "5": 0.0075,
            "10": 0.005,
            "20": 0.003,
            "25": 0.0025,
            "30": 0.001,
            "45": 0.0005
        },
        "batch-size": 32,
        "weight-decay": 0.005,
        "model-class-type": "linear",
        "model": "Sequential(\n  (0): Linear(in_features=784, out_features=512, bias=True)\n  (1): Dropout(p=0.005, inplace=False)\n  (2): ReLU()\n  (3): Linear(in_features=512, out_features=256, bias=True)\n  (4): Dropout(p=0.005, inplace=False)\n  (5): ReLU()\n  (6): Linear(in_features=256, out_features=128, bias=True)\n  (7): Dropout(p=0.005, inplace=False)\n  (8): ReLU()\n  (9): Linear(in_features=128, out_features=64, bias=True)\n  (10): Dropout(p=0.005, inplace=False)\n  (11): ReLU()\n  (12): Linear(in_features=64, out_features=32, bias=True)\n  (13): Dropout(p=0.005, inplace=False)\n  (14): ReLU()\n  (15): Linear(in_features=32, out_features=16, bias=True)\n  (16): Dropout(p=0.005, inplace=False)\n  (17): ReLU()\n  (18): Linear(in_features=16, out_features=10, bias=True)\n  (19): LogSoftmax(dim=1)\n)"
    },
    "Logs/MNIST-epochs=10-layers=7-2.pt": {
        "score": 0.9515,
        "max-score": 0.9532,
        "max-score-epoch": 8,
        "epochs": 10,
        "validation-results": {
            "1": 0.8989,
            "2": 0.8688,
            "3": 0.9242,
            "4": 0.9431,
            "5": 0.9294,
            "6": 0.9239,
            "7": 0.9319,
            "8": 0.9532,
            "9": 0.8959,
            "10": 0.9515
        },
        "learning-rate": {
            "1": 0.01,
            "5": 0.0075,
            "10": 0.005,
            "20": 0.003,
            "25": 0.0025,
            "30": 0.001,
            "45": 0.0005
        },
        "batch-size": 32,
        "weight-decay": 0.005,
        "model-class-type": "linear",
        "model": "Sequential(\n  (0): Linear(in_features=784, out_features=512, bias=True)\n  (1): Dropout(p=0.005, inplace=False)\n  (2): ReLU()\n  (3): Linear(in_features=512, out_features=256, bias=True)\n  (4): Dropout(p=0.005, inplace=False)\n  (5): ReLU()\n  (6): Linear(in_features=256, out_features=128, bias=True)\n  (7): Dropout(p=0.005, inplace=False)\n  (8): ReLU()\n  (9): Linear(in_features=128, out_features=64, bias=True)\n  (10): Dropout(p=0.005, inplace=False)\n  (11): ReLU()\n  (12): Linear(in_features=64, out_features=32, bias=True)\n  (13): Dropout(p=0.005, inplace=False)\n  (14): ReLU()\n  (15): Linear(in_features=32, out_features=16, bias=True)\n  (16): Dropout(p=0.005, inplace=False)\n  (17): ReLU()\n  (18): Linear(in_features=16, out_features=10, bias=True)\n  (19): LogSoftmax(dim=1)\n)"
    },
    "Logs/MNIST-epochs=1-layers=7-3.pt": {
        "score": 0.8482,
        "max-score": 0.8482,
        "max-score-epoch": 1,
        "epochs": 1,
        "validation-results": {
            "1": 0.8482
        },
        "learning-rate": {
            "1": 0.01,
            "5": 0.0075,
            "10": 0.005,
            "20": 0.003,
            "25": 0.0025,
            "30": 0.001,
            "45": 0.0005
        },
        "batch-size": 32,
        "weight-decay": 0.005,
        "model-class-type": "linear",
        "model": "Sequential(\n  (0): Linear(in_features=784, out_features=512, bias=True)\n  (1): Dropout(p=0.005, inplace=False)\n  (2): ReLU()\n  (3): Linear(in_features=512, out_features=256, bias=True)\n  (4): Dropout(p=0.005, inplace=False)\n  (5): ReLU()\n  (6): Linear(in_features=256, out_features=128, bias=True)\n  (7): Dropout(p=0.005, inplace=False)\n  (8): ReLU()\n  (9): Linear(in_features=128, out_features=64, bias=True)\n  (10): Dropout(p=0.005, inplace=False)\n  (11): ReLU()\n  (12): Linear(in_features=64, out_features=32, bias=True)\n  (13): Dropout(p=0.005, inplace=False)\n  (14): ReLU()\n  (15): Linear(in_features=32, out_features=16, bias=True)\n  (16): Dropout(p=0.005, inplace=False)\n  (17): ReLU()\n  (18): Linear(in_features=16, out_features=10, bias=True)\n  (19): LogSoftmax(dim=1)\n)"
    },
    "Logs/MNIST-epochs=1-layers=7-4.pt": {
        "score": 0.7111,
        "max-score": 0.7111,
        "max-score-epoch": 1,
        "epochs": 1,
        "validation-results": {
            "1": 0.7111
        },
        "learning-rate": {
            "1": 0.01,
            "5": 0.0075,
            "10": 0.005,
            "20": 0.003,
            "25": 0.0025,
            "30": 0.001,
            "45": 0.0005
        },
        "batch-size": 32,
        "weight-decay": 0.005,
        "model-class-type": "linear",
        "model": "Sequential(\n  (0): Linear(in_features=784, out_features=512, bias=True)\n  (1): Dropout(p=0.005, inplace=False)\n  (2): ReLU()\n  (3): Linear(in_features=512, out_features=256, bias=True)\n  (4): Dropout(p=0.005, inplace=False)\n  (5): ReLU()\n  (6): Linear(in_features=256, out_features=128, bias=True)\n  (7): Dropout(p=0.005, inplace=False)\n  (8): ReLU()\n  (9): Linear(in_features=128, out_features=64, bias=True)\n  (10): Dropout(p=0.005, inplace=False)\n  (11): ReLU()\n  (12): Linear(in_features=64, out_features=32, bias=True)\n  (13): Dropout(p=0.005, inplace=False)\n  (14): ReLU()\n  (15): Linear(in_features=32, out_features=16, bias=True)\n  (16): Dropout(p=0.005, inplace=False)\n  (17): ReLU()\n  (18): Linear(in_features=16, out_features=10, bias=True)\n  (19): LogSoftmax(dim=1)\n)"
    },
    "Logs/MNIST-epochs=50-layers=7-8.pt": {
        "score": 0.1135,
        "max-score": 0.1135,
        "max-score-epoch": 1,
        "epochs": 50,
        "validation-results": {
            "1": 0.1135,
            "2": 0.0958,
            "3": 0.1028,
            "4": 0.101,
            "5": 0.1135,
            "6": 0.1135,
            "7": 0.1028,
            "8": 0.1028,
            "9": 0.1135,
            "10": 0.1135,
            "11": 0.1135,
            "12": 0.1135,
            "13": 0.1135,
            "14": 0.1135,
            "15": 0.1135,
            "16": 0.1009,
            "17": 0.1135,
            "18": 0.1135,
            "19": 0.1135,
            "20": 0.1135,
            "21": 0.1009,
            "22": 0.1135,
            "23": 0.1135,
            "24": 0.1135,
            "25": 0.1135,
            "26": 0.1135,
            "27": 0.1135,
            "28": 0.1135,
            "29": 0.1135,
            "30": 0.1135,
            "31": 0.1135,
            "32": 0.1135,
            "33": 0.1135,
            "34": 0.1135,
            "35": 0.1135,
            "36": 0.1135,
            "37": 0.1135,
            "38": 0.1135,
            "39": 0.1135,
            "40": 0.1135,
            "41": 0.1135,
            "42": 0.1135,
            "43": 0.1135,
            "44": 0.1135,
            "45": 0.1135,
            "46": 0.1135,
            "47": 0.1135,
            "48": 0.1135,
            "49": 0.1135,
            "50": 0.1135
        },
        "learning-rate": {
            "10": 0.02,
            "25": 0.004,
            "40": 0.0008
        },
        "batch-size": 32,
        "weight-decay": 0.005,
        "model-class-type": "linear",
        "model": "Sequential(\n  (0): Linear(in_features=784, out_features=512, bias=True)\n  (1): Dropout(p=0.005, inplace=False)\n  (2): ReLU()\n  (3): Linear(in_features=512, out_features=256, bias=True)\n  (4): Dropout(p=0.005, inplace=False)\n  (5): ReLU()\n  (6): Linear(in_features=256, out_features=128, bias=True)\n  (7): Dropout(p=0.005, inplace=False)\n  (8): ReLU()\n  (9): Linear(in_features=128, out_features=64, bias=True)\n  (10): Dropout(p=0.005, inplace=False)\n  (11): ReLU()\n  (12): Linear(in_features=64, out_features=32, bias=True)\n  (13): Dropout(p=0.005, inplace=False)\n  (14): ReLU()\n  (15): Linear(in_features=32, out_features=16, bias=True)\n  (16): Dropout(p=0.005, inplace=False)\n  (17): ReLU()\n  (18): Linear(in_features=16, out_features=10, bias=True)\n  (19): LogSoftmax(dim=1)\n)"
    },
    "Logs/MNIST-epochs=5-layers=5-1.pt": {
        "score": 0.1135,
        "max-score": 0.1135,
        "max-score-epoch": 2,
        "epochs": 5,
        "validation-results": {
            "1": 0.1028,
            "2": 0.1135,
            "3": 0.1135,
            "4": 0.1135,
            "5": 0.1135
        },
        "learning-rate": {
            "3": 0.0015,
            "6": 0.00030000000000000003,
            "9": 6.000000000000001e-05
        },
        "batch-size": 2,
        "weight-decay": 0.005,
        "model-class-type": "conv2d",
        "model": "Sequential(\n  (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (2): ReLU()\n  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (5): ReLU()\n  (6): AvgPool2d(kernel_size=8, stride=8, padding=0)\n  (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (8): ReLU()\n  (9): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n  (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (11): ReLU()\n  (12): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n  (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (14): ReLU()\n  (15): Conv2d(32, 10, kernel_size=(1, 1), stride=(1, 1))\n  (16): LogSoftmax(dim=1)\n)"
    },
    "Logs/MNIST-epochs=1-layers=5-1.pt": {
        "score": 0.1117,
        "max-score": 0.1117,
        "max-score-epoch": 1,
        "epochs": 1,
        "validation-results": {
            "1": 0.1117
        },
        "learning-rate": {
            "3": 0.0015,
            "6": 0.00030000000000000003,
            "9": 6.000000000000001e-05
        },
        "batch-size": 64,
        "weight-decay": 0.005,
        "model-class-type": "conv2d",
        "model": "Sequential(\n  (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (2): ReLU()\n  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (5): ReLU()\n  (6): AvgPool2d(kernel_size=8, stride=8, padding=0)\n  (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (8): ReLU()\n  (9): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n  (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (11): ReLU()\n  (12): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n  (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (14): ReLU()\n  (15): Conv2d(32, 10, kernel_size=(1, 1), stride=(1, 1))\n  (16): LogSoftmax(dim=1)\n)"
    }
}